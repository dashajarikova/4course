{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Я взяла статьи с сайта https://cyberleninka.ru , ко всем были выделены ключевые слова"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_site_tangerine = 'листья / мандарин / числовые показатели'\n",
    "key_site_football = 'футбол / обучение'\n",
    "key_site_psycho = 'психология / история психологии / практика психологии / психотерапия'\n",
    "key_site_dogs = 'ягдтерьер / натаска / интеллект / охота / кровяной след / подранки'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавим выделенные мной ключевые слова для каждой статьи:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_tangerine = 'лист / мандарин / числовой показатель / лекарство '\n",
    "key_football = 'футбол / обучение / воспитание / тренер / поведение'\n",
    "key_psycho = 'психология / история психология / практика психология / психотерапия / ценность'\n",
    "key_dogs = 'ягдтерьер / натаска / интеллект / охота / кровяной след / подранки / собака / норная собака / кровь / след'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мне кажется, что для некоторых статей список ключевых слов были неполным, поэтому я буду опираться в том числе на добавленные мной слова?словосочетания"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/darazharikova/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from statistics import mean \n",
    "from textdistance import levenshtein\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('russian')\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "morph = MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_dict = {}\n",
    "texts_keys ={}\n",
    "\n",
    "with open('tangerine.txt', 'r', encoding='utf-8') as text:\n",
    "            text = text.read()\n",
    "            texts_dict['tangerine'] = text\n",
    "#             texts_keys [key_tangerine] = text\n",
    "            texts_keys ['tangerine'] = key_tangerine\n",
    "with open('football.txt', 'r', encoding='utf-8') as text:\n",
    "            text = text.read()\n",
    "            texts_dict['football'] = text\n",
    "#             texts_keys [key_football] = text\n",
    "            texts_keys ['football'] = key_football\n",
    "with open('psycho.txt', 'r', encoding='utf-8') as text:\n",
    "            text = text.read()\n",
    "            texts_dict['psycho'] = text\n",
    "            texts_keys ['psycho'] = key_psycho\n",
    "#             texts_keys [key_psycho] = text\n",
    "with open('dogs.txt', 'r', encoding='utf-8') as text:\n",
    "            text = text.read()\n",
    "            texts_dict['dogs'] = text\n",
    "#             texts_keys [key_psycho] = text\n",
    "            texts_keys ['dogs'] = key_dogs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Любимый препроцессинг"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "def preproc(text, stopwords, maximal=False):\n",
    "    stopwords = set(stopwords)\n",
    "    text = re.sub(r'\\n', r' ', text)\n",
    "    if maximal:\n",
    "        text = re.sub(r'[\"%0-9A-Za-z]', r'', text)\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    text = tokenizer.tokenize(text)\n",
    "    newtext = ''\n",
    "    for w in text:\n",
    "        if w not in stopwords: \n",
    "            lemma = morph.parse(w)[0].normal_form + ' '\n",
    "            newtext += lemma\n",
    "    return newtext\n",
    "\n",
    "preproc_texts_dict = {}\n",
    "for t in texts_dict.keys():\n",
    "    lemmatized = preproc(texts_dict[t], stop, maximal=False)\n",
    "    preproc_texts_dict[t] = lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preproc_texts_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keys_simple_search(keywords, text_dict):\n",
    "    for n in keywords.keys():\n",
    "        print(n)\n",
    "        text = text_dict[n]\n",
    "        keys = keywords[n]\n",
    "        keys = keys.split (' / ')\n",
    "        for kw in keys:\n",
    "            kw_c = text.count(kw)\n",
    "            print(f'{kw} - {kw_c}')\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tangerine\n",
      "листья - 0\n",
      "мандарин - 15\n",
      "числовые показатели - 0\n",
      "лекарство  - 0\n",
      "\n",
      "\n",
      "football\n",
      "футбол - 33\n",
      "обучение - 0\n",
      "воспитание - 3\n",
      "тренер - 6\n",
      "поведение - 3\n",
      "\n",
      "\n",
      "psycho\n",
      "психология - 10\n",
      "история психологии - 0\n",
      "практика психологии - 0\n",
      "психотерапия - 11\n",
      "ценность - 2\n",
      "\n",
      "\n",
      "dogs\n",
      "ягдтерьер - 15\n",
      "натаска - 2\n",
      "интеллект - 0\n",
      "охота - 4\n",
      "кровяной след - 22\n",
      "подранки - 0\n",
      "собака - 43\n",
      "норная собака - 0\n",
      "кровь - 12\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "keys_simple_search(texts_keys, preproc_texts_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что некоторые ключевые слова и словосочетания встречаются не так часто => интересно посмотреть, выделят ли их наши методы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install python-RAKE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import RAKE\n",
    "rake = RAKE.Rake(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproc_rake(text, maximal=False):\n",
    "    text = re.sub(r'\\n', r' ', text)\n",
    "    if maximal:\n",
    "        text = re.sub(r'[\"%0-9A-Za-z]', r'', text)\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    text = tokenizer.tokenize(text)\n",
    "    newtext = ''\n",
    "    for w in text:\n",
    "        lemma = morph.parse(w)[0].normal_form + ' '\n",
    "        newtext += lemma\n",
    "    return newtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "rake_preproc_texts_dict = {}\n",
    "for t in texts_dict.keys():\n",
    "    lemmatized = preproc_rake(texts_dict[t], maximal=False)\n",
    "    rake_preproc_texts_dict[t] = lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(pred, real):\n",
    "    tp = 0\n",
    "    fn = 0\n",
    "    fp = 0\n",
    "    #tn\n",
    "    for p in pred:\n",
    "        if p in real:\n",
    "            tp += 1 #3\n",
    "        if p not in real:\n",
    "            fp += 1 #2\n",
    "    for i in real:\n",
    "        if i not in pred:\n",
    "            fn += 1 #5\n",
    "    accuracy = tp / (tp + fn + fp)\n",
    "    precision = tp / (tp + fp) #3/3+2\n",
    "    recall = tp / (tp + fn)\n",
    "    try:\n",
    "        f1 = (2 * precision * recall) / (precision + recall)\n",
    "    except ZeroDivisionError:\n",
    "        f1 = 0.0\n",
    "    \n",
    "    print('accuracy: ' + str(accuracy) + '\\n' + 'precision: ' + str(precision) + '\\n' + 'f1: ' + str(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tangerine\n",
      "('р органический кислота', 9.0)\n",
      "('витамин', 1.0)\n",
      "\n",
      "\n",
      "football\n",
      "('спортивный воспитание футбол', 9.0)\n",
      "\n",
      "\n",
      "psycho\n",
      "('тенденция рисковать привести', 9.0)\n",
      "('являться носитель эталон', 9.0)\n",
      "('нашея профессией', 4.0)\n",
      "('активно навязывать', 4.0)\n",
      "('психический', 1.0)\n",
      "('нож', 1.0)\n",
      "('руководить', 1.0)\n",
      "('первое', 1.0)\n",
      "\n",
      "\n",
      "dogs\n",
      "('испытание прокладывание', 4.0)\n",
      "('проявление анонс', 4.0)\n",
      "('часовой давность', 4.0)\n",
      "('дильник', 1.0)\n",
      "('кровь', 1.0)\n",
      "('ягдтерьер', 1.0)\n",
      "('собака', 1.0)\n",
      "('область', 1.0)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "keys_rake_pred_dict = {}\n",
    "\n",
    "for text in texts_dict.keys():\n",
    "    print(text)\n",
    "    rake_kw_list = rake.run(preproc_texts_dict[text], maxWords=3, minFrequency=1)\n",
    "    kw_list = []\n",
    "    for kw in rake_kw_list:\n",
    "        print(kw)\n",
    "        kw_list.append(kw[0])\n",
    "    keys_rake_pred_dict[text] = kw_list\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['лист', 'мандарин', 'числовой показатель', 'лекарство ']\n",
      "['р органический кислота', 'витамин']\n",
      "accuracy: 0.0\n",
      "precision: 0.0\n",
      "f1: 0.0\n",
      "['футбол', 'обучение', 'воспитание', 'тренер', 'поведение']\n",
      "['спортивный воспитание футбол']\n",
      "accuracy: 0.0\n",
      "precision: 0.0\n",
      "f1: 0.0\n",
      "['психология', 'история психология', 'практика психология', 'психотерапия', 'ценность']\n",
      "['тенденция рисковать привести', 'являться носитель эталон', 'нашея профессией', 'активно навязывать', 'психический', 'нож', 'руководить', 'первое']\n",
      "accuracy: 0.0\n",
      "precision: 0.0\n",
      "f1: 0.0\n",
      "['ягдтерьер', 'натаска', 'интеллект', 'охота', 'кровяной след', 'подранки', 'собака', 'норная собака', 'кровь', 'след']\n",
      "['испытание прокладывание', 'проявление анонс', 'часовой давность', 'дильник', 'кровь', 'ягдтерьер', 'собака', 'область']\n",
      "accuracy: 0.2\n",
      "precision: 0.3\n",
      "f1: 0.33333333333333326\n"
     ]
    }
   ],
   "source": [
    "def do_metrics(pred_dict):\n",
    "    global texts_keys\n",
    "    for text in texts_keys.keys():\n",
    "        keys_real = texts_keys[text]\n",
    "        keys_real = keys_real.split (' / ')\n",
    "        print (keys_real)\n",
    "        keys_pred = pred_dict[text]\n",
    "        print (keys_pred)\n",
    "        do_metrics = metrics (keys_real,keys_pred)\n",
    "    return\n",
    "do_metrics(keys_rake_pred_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что rake выделяет часто словосочетания. Для статьи football это даже неплохо получилось, но для статьи psycho не очень - возможно потому что статья сама по себе очень разнообразна. (я попыталась подправить ситацию со словосоетаниями, чтобы лучше работали метрики, но юпитер ломается на этом месте каждый раз. Пишу чтобы вы не подумали, что я не вижу проблему с подсчетом точности и тд)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Textrank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.summarization import keywords as kw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tangerine\n",
      "плод\t0.20636922780727826\n",
      "которыи\t0.18127954548032452\n",
      "род цитрус мандарин\t0.1687535930777063\n",
      "лист\t0.1379554152191496\n",
      "также\t0.10829621999634018\n",
      "\n",
      "\n",
      "football\n",
      "спортивныи\t0.2356606420445973\n",
      "спорт\t0.16183141593875067\n",
      "юныи\t0.1605121572012145\n",
      "профессиональныи футболист это личность\t0.15933995762440142\n",
      "свои\t0.15897264592256943\n",
      "\n",
      "\n",
      "psycho\n",
      "которыи\t0.2826962774371547\n",
      "это\t0.22330098740377044\n",
      "психология\t0.20195192739453083\n",
      "психотерапия\t0.17584963333746262\n",
      "направление подход\t0.14319714341420411\n",
      "\n",
      "\n",
      "dogs\n",
      "след\t0.33100144433714807\n",
      "охотничии собака\t0.2842191846832642\n",
      "это\t0.1832984036145123\n",
      "ягдтерьер\t0.15838015601436595\n",
      "кровь\t0.14551358782885038\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "keys_textrank_pred_dict = {}\n",
    "\n",
    "for text in texts_dict.keys():\n",
    "    print(text)\n",
    "    text_rank = kw(preproc_texts_dict[text], pos_filter=[], scores=True)\n",
    "    kw_list = []\n",
    "    for k_w in text_rank[:5]:\n",
    "        print(f'{k_w[0]}\\t{k_w[1]}')\n",
    "        kw_list.append(k_w[0])\n",
    "    keys_textrank_pred_dict[text] = kw_list\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['лист', 'мандарин', 'числовой показатель', 'лекарство ']\n",
      "['плод', 'которыи', 'род цитрус мандарин', 'лист', 'также']\n",
      "accuracy: 0.125\n",
      "precision: 0.25\n",
      "f1: 0.22222222222222224\n",
      "['футбол', 'обучение', 'воспитание', 'тренер', 'поведение']\n",
      "['спортивныи', 'спорт', 'юныи', 'профессиональныи футболист это личность', 'свои']\n",
      "accuracy: 0.0\n",
      "precision: 0.0\n",
      "f1: 0.0\n",
      "['психология', 'история психология', 'практика психология', 'психотерапия', 'ценность']\n",
      "['которыи', 'это', 'психология', 'психотерапия', 'направление подход']\n",
      "accuracy: 0.25\n",
      "precision: 0.4\n",
      "f1: 0.4000000000000001\n",
      "['ягдтерьер', 'натаска', 'интеллект', 'охота', 'кровяной след', 'подранки', 'собака', 'норная собака', 'кровь', 'след']\n",
      "['след', 'охотничии собака', 'это', 'ягдтерьер', 'кровь']\n",
      "accuracy: 0.25\n",
      "precision: 0.3\n",
      "f1: 0.4\n"
     ]
    }
   ],
   "source": [
    "do_metrics(keys_textrank_pred_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тоже неплохо! Но здесь, кажется, попадает больше шелухи (как например в dogs \"это\" выше чем \"собака\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf-Idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = list(preproc_texts_dict.values())\n",
    "\n",
    "cv=CountVectorizer(max_features=10000) #stop_words=stopwords,\n",
    "word_count_vector=cv.fit_transform(corpus)\n",
    "\n",
    "tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\n",
    "tf_idf_vector=tfidf_transformer.fit_transform(word_count_vector)\n",
    "\n",
    "df = pd.DataFrame(tf_idf_vector.toarray(), columns=cv.get_feature_names(), index=texts_dict.keys())\n",
    "\n",
    "stop = set(stop)\n",
    "words = set(df.columns)\n",
    "\n",
    "stop_words = stop.intersection(words)\n",
    "for w in stop_words:\n",
    "    df = df.drop([w], axis=1)\n",
    "\n",
    "df = df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tangerine\n",
      "мандарин    0.514641\n",
      "плод        0.257320\n",
      "лист        0.183800\n",
      "кожура      0.147040\n",
      "который     0.134280\n",
      "citrus      0.110280\n",
      "витамин     0.110280\n",
      "Name: tangerine, dtype: float64\n",
      "\n",
      "\n",
      "football\n",
      "футболист     0.398241\n",
      "футбольный    0.284458\n",
      "юный          0.284458\n",
      "футбол        0.256012\n",
      "спортивный    0.246697\n",
      "спорт         0.170675\n",
      "спортсмен     0.170675\n",
      "Name: football, dtype: float64\n",
      "\n",
      "\n",
      "psycho\n",
      "психотерапия    0.281638\n",
      "психология      0.256034\n",
      "направление     0.201860\n",
      "который         0.187053\n",
      "это             0.179766\n",
      "заложить        0.153621\n",
      "подход          0.153621\n",
      "Name: psycho, dtype: float64\n",
      "\n",
      "\n",
      "dogs\n",
      "собака       0.492642\n",
      "след         0.469728\n",
      "кровяной     0.263506\n",
      "испытание    0.229136\n",
      "охотничий    0.171852\n",
      "ягдтерьер    0.171852\n",
      "работа       0.138942\n",
      "Name: dogs, dtype: float64\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "col_names = list(df.columns)\n",
    "\n",
    "keys_pred_tfidf_dict = {}\n",
    "\n",
    "for col in col_names:\n",
    "#     len_k = len(texts_keys[col].split (' / '))\n",
    "    kws = df.nlargest(7, col)\n",
    "    kws = kws[col]\n",
    "    print(col)\n",
    "    print(kws)\n",
    "    keys_pred_tfidf_dict[col] = list(kws.index)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['лист', 'мандарин', 'числовой показатель', 'лекарство ']\n",
      "['мандарин', 'плод', 'лист', 'кожура', 'который', 'citrus', 'витамин']\n",
      "accuracy: 0.2222222222222222\n",
      "precision: 0.5\n",
      "f1: 0.36363636363636365\n",
      "['футбол', 'обучение', 'воспитание', 'тренер', 'поведение']\n",
      "['футболист', 'футбольный', 'юный', 'футбол', 'спортивный', 'спорт', 'спортсмен']\n",
      "accuracy: 0.09090909090909091\n",
      "precision: 0.2\n",
      "f1: 0.16666666666666666\n",
      "['психология', 'история психология', 'практика психология', 'психотерапия', 'ценность']\n",
      "['психотерапия', 'психология', 'направление', 'который', 'это', 'заложить', 'подход']\n",
      "accuracy: 0.2\n",
      "precision: 0.4\n",
      "f1: 0.3333333333333333\n",
      "['ягдтерьер', 'натаска', 'интеллект', 'охота', 'кровяной след', 'подранки', 'собака', 'норная собака', 'кровь', 'след']\n",
      "['собака', 'след', 'кровяной', 'испытание', 'охотничий', 'ягдтерьер', 'работа']\n",
      "accuracy: 0.21428571428571427\n",
      "precision: 0.3\n",
      "f1: 0.3529411764705882\n"
     ]
    }
   ],
   "source": [
    "do_metrics(keys_pred_tfidf_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим что здесь вообще всё еще лучше! Но статья psycho опять страдает"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Выводы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Несмотря на плачевные результаты метрик, методы справились неплохо. Лушче всего tf-idf, затем textrank и rake на 3м месте.\n",
    "2) Rake больше остальных склонен к выделению ключевых последовательностей (больше 1 слова), возможно в этом его проблема. Хотя на бОльших текстах это может сработать лучше. А tf-idf выделяет только по 1 ключевому слову и для наших простеньких статей этого достаточно\n",
    "3) В статье psycho использовалось много синонимов и тд - вот у нее везде и самых плохой результат, а в dogs использовались незаменимые слова (порода собаки \"ягдтерьер\", термин \"кровяной след\" и тд) и эту статью все методы более или менее хорошо обрабатывали."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
